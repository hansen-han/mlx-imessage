{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlx-imessage\n",
    "**Author**: Hansen Han   \n",
    "**Date**: September 8th, 2024   \n",
    "\n",
    "**Objective:**  \n",
    "The goal of this project is to locally fine-tune a large language model (LLM) using iMessage chat history to replicate your personal communication patterns. \n",
    "\n",
    "Doing fine-tuning directly on the device ensure privacy and avoiding data leakage to online services. The project focuses on maintaining data security while achieving a personalized, conversational AI that matches your own writing style / text to help come up with organic messages.\n",
    "\n",
    "**Sources / Inspiration:**\n",
    "- https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/qlora-mlx: Local Fine-tuning on Mac (QLoRA with MLX)\n",
    "- https://github.com/ml-explore/mlx-examples/tree/main/lora:Fine-Tuning with LoRA or QLoRA\n",
    "- https://github.com/gavi/mlx-whatsapp:  An mlx project to train a base model on your whatsapp chats using (Q)Lora finetuning\n",
    "- https://github.com/ishan0102/iClone: Clone your friends with iMessage and MLX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries / Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import json\n",
    "import subprocess\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "pd.set_option('display.max_rows', 100)  # Display all rows\n",
    "pd.set_option('display.max_columns', 25)  # Display all columns\n",
    "\n",
    "db_path = \"/path/to/chat.db\" # change this to your path\n",
    "your_name = \"Your Name\" # change this to your name\n",
    "\n",
    "# set the model to use (see: https://huggingface.co/mlx-community for other models available)\n",
    "model_path = \"mlx-community/Mistral-7B-Instruct-v0.2-4bit\"\n",
    "adapter_path = \"adapters.npz\"\n",
    "\n",
    "max_tokens = 500\n",
    "max_tokens_str = str(max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command_with_live_output(command: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Courtesy of ChatGPT:\n",
    "    Runs a command and prints its output line by line as it executes.\n",
    "\n",
    "    Args:\n",
    "        command (List[str]): The command and its arguments to be executed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Print the output line by line\n",
    "    while True:\n",
    "        output = process.stdout.readline()\n",
    "        if output == '' and process.poll() is not None:\n",
    "            break\n",
    "        if output:\n",
    "            print(output.strip())\n",
    "        \n",
    "    # Print the error output, if any\n",
    "    err_output = process.stderr.read()\n",
    "    if err_output:\n",
    "        print(err_output)\n",
    "\n",
    "def construct_shell_command(command: list[str]) -> str:\n",
    "    return str(command).replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\",\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Process iMessage Chat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database and load the tables that you need\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "query = \"SELECT * FROM chat;\"\n",
    "chat_table = pd.read_sql_query(query, conn)\n",
    "\n",
    "query = \"SELECT * FROM message;\"\n",
    "messages_table = pd.read_sql_query(query, conn)\n",
    "\n",
    "query = \"SELECT * FROM chat_message_join;\"\n",
    "chat_message_join = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# merge the tables together so that you get data you need (the text message, who is it from, did I send it?)\n",
    "all_data = messages_table[['ROWID', 'text', 'is_from_me']].merge(chat_message_join, left_on=\"ROWID\", right_on=\"message_id\")[['text', 'is_from_me', 'chat_id', 'message_id', 'message_date']].merge(chat_table[['ROWID', 'chat_identifier', 'guid']], left_on='chat_id', right_on=\"ROWID\")\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove groupchats (this could be very confusing)\n",
    "filtered_data = all_data[~all_data['guid'].str.contains(';chat')]\n",
    "filtered_data = filtered_data[['text', 'is_from_me', 'message_date', 'chat_identifier']]\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only look at chats where I've sent more than 100 messages to the person (actual relationships)\n",
    "# this was something I expermented with, you can ignore this or look at other ones! \n",
    "sum_is_from_me = filtered_data.groupby('chat_identifier')['is_from_me'].sum().reset_index()\n",
    "sum_is_from_me\n",
    "\n",
    "# look at the numbers that I probably have an actual relationship with\n",
    "sum_is_from_me[sum_is_from_me.is_from_me > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at those identifiers \n",
    "chat_identifiers_to_use = list(sum_is_from_me[sum_is_from_me.is_from_me > 100]['chat_identifier'])\n",
    "chat_identifiers_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires manual annotation: give each number a name, and define a relationship for each name\n",
    "# with imessage, you may have duplicates, so this is helpful \n",
    "\n",
    "number_to_name_map = {\n",
    "    \"+134289190384918319489134\": \"NAME1\"\n",
    "}\n",
    "\n",
    "name_to_relationship_map = {\n",
    "    \"NAME1\": \"Friend\",\n",
    "    \"NAME2\": \"Parent\",\n",
    "    \"NAME3\": \"Boss\",\n",
    "}\n",
    "# map and add the name and relationship data to the message data \n",
    "prep_df = filtered_data[filtered_data.chat_identifier.isin(chat_identifiers_to_use)]\n",
    "prep_df['name'] = prep_df['chat_identifier'].map(number_to_name_map)\n",
    "prep_df['relationship'] = prep_df['name'].map(name_to_relationship_map)\n",
    "prep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_result_dict = {}\n",
    "for name in prep_df[prep_df.relationship == \"Friend\"]['name'].unique(): # only look at friends to start with\n",
    "    subset_df = prep_df[prep_df.name == name][~prep_df.text.isin([\"None\", None, \"\"])]\n",
    "    # we need to split these into questions and answers.\n",
    "    messages = []\n",
    "    replies = []\n",
    "\n",
    "    last_symbol = subset_df.iloc[0]['is_from_me']\n",
    "    current_text = subset_df.iloc[0]['text']\n",
    "    working_text = current_text\n",
    "\n",
    "\n",
    "    # skip the first message since we've already extracted it\n",
    "    for x in range(1, len(subset_df)):\n",
    "        current_symbol = subset_df.iloc[x]['is_from_me']\n",
    "        current_text = subset_df.iloc[x]['text']\n",
    "\n",
    "        # if it has switched, record and wipe the last message/reply\n",
    "        if current_symbol != last_symbol:\n",
    "            if last_symbol == 0:\n",
    "                messages.append(working_text)\n",
    "                working_text = current_text\n",
    "            else:\n",
    "                # add a signature if it is the end of a reply\n",
    "                working_text = working_text\n",
    "                replies.append(working_text)\n",
    "                working_text = current_text\n",
    "        else:\n",
    "            working_text = working_text + \"\\n\" + current_text\n",
    "        \n",
    "        last_symbol = current_symbol\n",
    "\n",
    "    # save results\n",
    "    name_result_dict[name] = {\"messages\": messages, \"replies\": replies}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "intstructions_string = f\"\"\"\n",
    "You are a chatbot trained on {your_name}'s message history. Your goal is to communicate in a tone and style that closely matches {your_name}'s natural way of speaking.\n",
    "Please reply to the following message from your\n",
    "\"\"\"\n",
    "\n",
    "prompts = []\n",
    "\n",
    "prompt_template = lambda comment, response: f'''<s>[INST] {intstructions_string} {relationship}: \\n{comment} \\n[/INST]\\n''' + response + \"</s>\"\n",
    "\n",
    "for name in list(name_result_dict.keys()):\n",
    "    messages = name_result_dict[name]['messages']\n",
    "    replies = name_result_dict[name]['replies']\n",
    "    relationship = name_to_relationship_map[name]\n",
    "\n",
    "    for i in range(len(messages)):\n",
    "        try:\n",
    "            prompt = {\"text\":prompt_template(messages[i], relationship, replies[i])}\n",
    "            prompts.append(prompt)\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test and val data\n",
    "num_test = 10\n",
    "num_val = 10\n",
    "test_val_index_list = random.sample(range(0, len(prompts)-1), num_test+num_val)\n",
    "\n",
    "test_list = [prompts[index] for index in test_val_index_list[:num_test]]\n",
    "val_list = [prompts[index] for index in test_val_index_list[num_test:]]\n",
    "\n",
    "for example in test_list+val_list:\n",
    "    prompts.remove(example)\n",
    "\n",
    "### save data to files\n",
    "with open('./data/train.jsonl', 'w') as output_file:\n",
    "    for prompt in prompts:\n",
    "        json.dump(prompt, output_file)\n",
    "        output_file.write('\\n')\n",
    "\n",
    "with open('./data/test.jsonl', 'w') as output_file:\n",
    "    for prompt in test_list:\n",
    "        json.dump(prompt, output_file)\n",
    "        output_file.write('\\n')\n",
    "\n",
    "with open('./data/valid.jsonl', 'w') as output_file:\n",
    "    for prompt in val_list:\n",
    "        json.dump(prompt, output_file)\n",
    "        output_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Model w/ mlx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from HF\n",
    "model, tokenizer = load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try running inference without fine tuning\n",
    "prompt_builder = lambda comment, relationship: f'''<s>[INST] {intstructions_string} {relationship}: \\n{comment} \\n[/INST]\\n'''\n",
    "prompt = prompt_builder(\"Hey man, whats up?\")\n",
    "response = generate(model, tokenizer, prompt=prompt, max_tokens = max_tokens,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create command for fine-tuning with QLoRA\n",
    "num_iters = \"100\"\n",
    "steps_per_eval = \"10\"\n",
    "val_batches = \"-1\"\n",
    "learning_rate = \"1e-5\" \n",
    "num_layers = 16 \n",
    "\n",
    "command = ['python', 'scripts/lora.py', '--model', model_path, '--train', '--iters', num_iters, '--steps-per-eval', steps_per_eval, '--val-batches', val_batches, '--learning-rate', learning_rate, '--lora-layers', num_layers, '--test']\n",
    "print(construct_shell_command(command)) # run this command in the terminal to generate the adapters.npz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try running inference with your fine-tuned model and it should reply with your tone and style.\n",
    "context = \"This is your friend, Adam.\" # After experimenting, I found that adding context improved coherence. \n",
    "relationship = \"friend\"\n",
    "\n",
    "instructions_string = f\"\"\"\n",
    "You are a chatbot trained on {your_name}'s message history. Your goal is to communicate in a tone and style that closely matches {your_name}'s natural way of speaking.\n",
    "\n",
    "{context}\n",
    "\n",
    "Please reply to the following message from your {relationship}:\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = lambda comment: f'''<s>[INST] {instructions_string} \\n{comment} \\n[/INST]\\n'''\n",
    "\n",
    "comment = \"Hey buddy, do you want to hang out soon?\"\n",
    "prompt = prompt_builder(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define command\n",
    "command = ['python', 'scripts/lora.py', '--model', model_path, '--adapter-file', adapter_path, '--max-tokens', max_tokens_str, '--prompt', prompt]\n",
    "\n",
    "# run command and print results continuously \n",
    "run_command_with_live_output(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
